---
layout: post
title: "How to Spot AI Enshittification"
date: 2025-10-21
tags: [AI, technology, enshittification, Doctorow, ethics, platforms]
---

Artificial intelligence is in its â€œgood to usersâ€ phase â€” powerful, accessible, and often free.  
But history shows that platforms rarely stay that way. As Cory Doctorow warns, once a technology gains dominance, it often enters a cycle of **enshittification** â€” prioritizing profit over users until the product degrades.

This post outlines ten early warning signs that AI systems may be following the same path.

---

## âš ï¸ 10 Warning Signs of AI Enshittification

### 1. Sponsored or Biased Answers Begin to Creep In
When AI responses start favoring paid partners or â€œrecommendedâ€ brands without clear disclosure.  
> Example: An assistant suggests one travel site over another because itâ€™s a paid partner.

---

### 2. Tiered Access to â€œRealâ€ Intelligence
Free users get throttled models while premium users retain quality.  
> Creates information inequality and pressures users into paywalls.

---

### 3. Increased Friction for Switching or Exporting Data
AI platforms make it hard to export chat history, fine-tunes, or projects.  
> Lock-in comes first; extraction follows.

---

### 4. Loss of Transparency or Auditability
Less visibility into data sources, model versions, or influences behind answers.  
> Opacity enables hidden monetization and bias.

---

### 5. Hidden or Expanding Surveillance
Platforms quietly collect more user data â€œto improve performance.â€  
> Shifts from serving users to monetizing them.

---

### 6. Degradation of the Free Tier
Free versions become slower, inaccurate, or stripped of features â€” not due to cost, but to drive upgrades.  
> Echoes social mediaâ€™s shift from user reach to pay-to-play.

---

### 7. Monetization Over User Benefit
Product design choices prioritize ads, upsells, or partnerships over usefulness.  
> Sponsored content in AI output is a classic symptom.

---

### 8. User Exploitation Without Clear Consent
User data, conversations, or uploads are used for model training or shared with partners without explicit opt-in.  
> The user becomes an unpaid contributor.

---

### 9. Declining Accuracy or Usefulness Over Time
As models become optimized for corporate interests, responses lose clarity and trustworthiness.  
> Utility quietly erodes.

---

### 10. Community and Developer Ecosystem Suppressed
APIs close, plug-ins vanish, and open-source competition is throttled.  
> Innovation and user autonomy fade as ecosystems are walled off.

---

## ğŸ§­ How to Spot the Shift Early

- Compare answers across multiple AI systems regularly.  
- Watch for pricing and tier changes.  
- Look for â€œsponsoredâ€ or â€œpartneredâ€ language in responses.  
- Read Terms of Service updates carefully â€” words like *personalization* or *improvement* often mean expanded data use.  
- Track the subtle downgrades: slower speed, less detail, fewer export options.

---

## ğŸ“š References

- Cory Doctorow, ["Tiktokâ€™s Enshittification"](https://pluralistic.net/2023/01/21/potemkin-ai/#enshittification), 2023  
- WIRED, ["Can AI Avoid the Enshittification Trap?"](https://www.wired.com/story/ai-enshittification-trap/), 2024  

---

*This article is part of an ongoing series on technology ethics and AI design integrity.*  
*Originally inspired by WIREDâ€™s coverage and Doctorowâ€™s framework.*
